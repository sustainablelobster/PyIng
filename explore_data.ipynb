{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ingredient embedding\n",
    "\n",
    "\n",
    "The end goal is wanting to create recipe embeddings that include quantity of each of the ingredients.\n",
    "\n",
    "To achieve this ingredient names need to be parsed. There are several commercial tools available, vbut I am not willing to pay for an API and there are no other easily usable modules in python. The solution?? Make one!\n",
    "\n",
    "I have downloaded ingredient data from the NYTimes model which was used to train a model to extract information such as unit, quantity and ingredient name from an ingredient such as \"3 yellow squashes, roughly chopped\".\n",
    "\n",
    "I initially thought about this problem as one of a translation problem,  where each term in the ingredient could be labelled as either unit/ingredient etc.\n",
    "This is actually not that useful as the are complexities within the phrasing that make extracting the actual quantity or the unit from the labelled words difficult. For instance:\n",
    "\n",
    "\"2 tablespoons and 1 teaspoon of sugar\" Here there are actually two quantities and units. This labelled dataset does not proide this information so the model would not actually be able to learn to label both. Instead, the label is given as \"7 teaspoons\".\n",
    "\n",
    "Instead I will aim to train a combined model with a common ingredient embedding. It is a combined model as there will be two seperate \"tails\" to the model for the two types of output data:\n",
    "\n",
    "### Model Input: \n",
    "\n",
    "\"2 tablespoons and 1 teaspoon of white sugar\" -> Remove punctuation\n",
    "Word Vectorisation -> \n",
    "\n",
    "[3, 12, 456, 34, 2304, 304, 78, 6529, 489]\n",
    "\n",
    "Padding to longest sequence -> \n",
    "\n",
    "[3, 12, 456, 34, 2304, 304, 78, 6529, 489, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "### Label:\n",
    "\n",
    "[\n",
    "\n",
    "    [\n",
    "        [0 0 0 0 0 0 1 1],      Ingredient Binary Mask\n",
    "        [0 0 0 0 1 0 0 0]       Unit Binary Mask\n",
    "    ],\n",
    "    [\n",
    "        7.0,                    Quantity Scalar\n",
    "        0                       Range End (if there is \"2 to 4 pears\" this would be 4)\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "### Model:\n",
    "\n",
    "Embedding Layer (pretrained Glove Embeddings)\n",
    "\n",
    "LSTM layers\n",
    "\n",
    "Dense Layers\n",
    "\n",
    "\n",
    "#### Model Tail 1:\n",
    "Loss: Binary Crossentropy\n",
    "\n",
    "Metrics: F1 score for unit and name seperatly\n",
    "\n",
    "Used as a binary classifier on the input ingredient sentence, where each word is classified for it being an ingredient or not an ingredient. The same is done for the unit and the outputs are stacked:\n",
    "\n",
    "Given \"2 tablespoons and 1 teaspoon of white sugar\" it will return [0 0 0 0 0 0 1 1] in the first row to indicate the ingredient and [0 0 0 0 1 0 0 0] in the second dimension to indicate unit.\n",
    "\n",
    "#### Model Tail 2:\n",
    "Loss: Huber Loss\n",
    "\n",
    "Metrics: MSE for the quantity and range end seperatly\n",
    "\n",
    "Used for a regression on the embedding of the last dense layer, to predict the quantity and the range of the ingredient. \n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Data Loading and Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179207 entries, 0 to 179206\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   input      179063 non-null  object \n",
      " 1   name       178759 non-null  object \n",
      " 2   qty        179207 non-null  float64\n",
      " 3   range_end  173986 non-null  float64\n",
      " 4   unit       123082 non-null  object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load in the labelled ingredient data provided by NYT\n",
    "\n",
    "ing_df = pd.read_csv(\"nyt-ingredients-snapshot-2015.csv\", index_col=\"index\")\n",
    "\n",
    "# Drop the columns not needed\n",
    "ing_df.drop([\"comment\"], axis=1, inplace=True)\n",
    "ing_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "A few bits of preprocessing need to happen to the data first\n",
    "\n",
    "1. Data Cleaning\n",
    "- Removing punctuation except for . and , as these are common and have contextual meaning\n",
    "- making . and , seperate words\n",
    "- Checking that for each row in the dataset the name of the ingredient and the unit are in the input (these are given seperately)\n",
    "- Removing null inputs (where there is no entry for the input and``  ingredient)\n",
    "- Dropping rows that dont have a quantity\n",
    "- replacing nan values in the unit column with 0's\n",
    "\n",
    "2. Creating labels\n",
    "- Converting the data to the relevant output (regexp)\n",
    "- Dropping rows that havent matched to the regexp or dont contain a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing na values in the name an input, the length of the df is now 178668\n",
      "After removing rows where the name or the input contain commas or full stops, the length of the df is now 174983\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "# Check if the name column has any punctuation - It does have a few rows that have way too much information, I will remove these\n",
    "def check_if_punct(string: str):\n",
    "    b = True\n",
    "    punc_regexp = r\"[.,]\"\n",
    "    match = re.search(punc_regexp, string)\n",
    "    if match:\n",
    "        b = np.nan\n",
    "    return b\n",
    "\n",
    "# Remove rows that dont have the required fields\n",
    "ing_df.dropna(subset=['input', \"name\"], inplace=True)\n",
    "print(f\"After removing na values in the name an input, the length of the df is now {len(ing_df)}\")\n",
    "\n",
    "# Convert all the string columns to strings\n",
    "for col in ['input', 'name', 'unit']:\n",
    "    ing_df[col] = ing_df[col].apply(str)\n",
    "    \n",
    "# Replace all na values in the qty column with 0\n",
    "ing_df[\"qty\"] = ing_df[\"qty\"].fillna(0)\n",
    "ing_df[\"range_end\"] = ing_df[\"range_end\"].fillna(0)\n",
    "\n",
    "ing_df['name_no_punct'] = ing_df['name'].apply(check_if_punct)\n",
    "\n",
    "# Remove rows that have punctuation in the\n",
    "ing_df.dropna(subset=[\"name_no_punct\"], inplace=True)\n",
    "print(f\"After removing rows where the name or the input contain commas or full stops, the length of the df is now {len(ing_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that the filter is working and that rows are being removed. It is also good to see that this filter does not drastically affect the number of rows.\n",
    "\n",
    "After all the filters have been compelted, then I will manually review subsets of the data to see if there are any further issues or things that have slipped through the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>name</th>\n",
       "      <th>qty</th>\n",
       "      <th>range_end</th>\n",
       "      <th>unit</th>\n",
       "      <th>name_no_punct</th>\n",
       "      <th>input_parsed</th>\n",
       "      <th>name_parsed</th>\n",
       "      <th>unit_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 1/4 cups cooked and pureed fresh butternut s...</td>\n",
       "      <td>butternut squash</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cup</td>\n",
       "      <td>True</td>\n",
       "      <td>1 1 / 4 cups cooked and pureed fresh butternut...</td>\n",
       "      <td>butternut squash</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 cup peeled and cooked fresh chestnuts (about...</td>\n",
       "      <td>chestnuts</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cup</td>\n",
       "      <td>True</td>\n",
       "      <td>1 cup peeled and cooked fresh chestnuts about ...</td>\n",
       "      <td>chestnuts</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 medium-size onion, peeled and chopped</td>\n",
       "      <td>onion</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>1 medium size onion , peeled and chopped</td>\n",
       "      <td>onion</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 stalks celery, chopped coarse</td>\n",
       "      <td>celery</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stalk</td>\n",
       "      <td>True</td>\n",
       "      <td>2 stalks celery , chopped coarse</td>\n",
       "      <td>celery</td>\n",
       "      <td>stalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 1/2 tablespoons vegetable oil</td>\n",
       "      <td>vegetable oil</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tablespoon</td>\n",
       "      <td>True</td>\n",
       "      <td>1 1 / 2 tablespoons vegetable oil</td>\n",
       "      <td>vegetable oil</td>\n",
       "      <td>tablespoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input              name  \\\n",
       "index                                                                        \n",
       "0      1 1/4 cups cooked and pureed fresh butternut s...  butternut squash   \n",
       "1      1 cup peeled and cooked fresh chestnuts (about...         chestnuts   \n",
       "2                1 medium-size onion, peeled and chopped             onion   \n",
       "3                        2 stalks celery, chopped coarse            celery   \n",
       "4                        1 1/2 tablespoons vegetable oil     vegetable oil   \n",
       "\n",
       "        qty  range_end        unit name_no_punct  \\\n",
       "index                                              \n",
       "0      1.25        0.0         cup          True   \n",
       "1      1.00        0.0         cup          True   \n",
       "2      1.00        0.0         nan          True   \n",
       "3      2.00        0.0       stalk          True   \n",
       "4      1.50        0.0  tablespoon          True   \n",
       "\n",
       "                                            input_parsed       name_parsed  \\\n",
       "index                                                                        \n",
       "0      1 1 / 4 cups cooked and pureed fresh butternut...  butternut squash   \n",
       "1      1 cup peeled and cooked fresh chestnuts about ...         chestnuts   \n",
       "2               1 medium size onion , peeled and chopped             onion   \n",
       "3                       2 stalks celery , chopped coarse            celery   \n",
       "4                      1 1 / 2 tablespoons vegetable oil     vegetable oil   \n",
       "\n",
       "      unit_parsed  \n",
       "index              \n",
       "0             cup  \n",
       "1             cup  \n",
       "2             nan  \n",
       "3           stalk  \n",
       "4      tablespoon  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Preprocess the input\n",
    "\n",
    "def process_punctuation(string):\n",
    "    string = re.sub(r\"([,./])\", r\" \\g<1> \", string)\n",
    "    string = re.sub(r'[^\\w\\s.,/]', r\" \", string)\n",
    "    string = re.sub(r\"\\s+\", r\" \", string)\n",
    "    string = re.sub(\"(\\s$)|(^\\s)\", \"\", string)\n",
    "\n",
    "    string = string.lower()\n",
    "    # Remove trailing white space\n",
    "    if string[-1] == \" \":\n",
    "        string = string[:-1]\n",
    "    if string[0] == \" \":\n",
    "        string = string[1:]\n",
    "    return string\n",
    "\n",
    "\n",
    "for col in [\"input\", \"name\", \"unit\"]:\n",
    "    ing_df[col + '_parsed'] = ing_df[col].apply(process_punctuation)\n",
    "\n",
    "\n",
    "ing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above the string columns are processed to ensure they are all lower case and also to remove any punctuation. The only punctuation I am including is . and , as these are used regularly and provide useful contextual information. finally I am removing multiple whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>name</th>\n",
       "      <th>qty</th>\n",
       "      <th>range_end</th>\n",
       "      <th>unit</th>\n",
       "      <th>name_no_punct</th>\n",
       "      <th>input_parsed</th>\n",
       "      <th>name_parsed</th>\n",
       "      <th>unit_parsed</th>\n",
       "      <th>check_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2 to 3 teaspoons minced jalapeño</td>\n",
       "      <td>jalapeños</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>teaspoon</td>\n",
       "      <td>True</td>\n",
       "      <td>2 to 3 teaspoons minced jalapeño</td>\n",
       "      <td>jalapeños</td>\n",
       "      <td>teaspoon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Salt and freshly ground black pepper to taste</td>\n",
       "      <td>Salt and black pepper</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>salt and freshly ground black pepper to taste</td>\n",
       "      <td>salt and black pepper</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Salt and freshly ground black pepper</td>\n",
       "      <td>Salt and black pepper</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>salt and freshly ground black pepper</td>\n",
       "      <td>salt and black pepper</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Salt and freshly ground black pepper</td>\n",
       "      <td>Salt and black pepper</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>salt and freshly ground black pepper</td>\n",
       "      <td>salt and black pepper</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Salt and freshly ground pepper</td>\n",
       "      <td>Salt and pepper</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>salt and freshly ground pepper</td>\n",
       "      <td>salt and pepper</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input                   name  \\\n",
       "index                                                                         \n",
       "253                 2 to 3 teaspoons minced jalapeño              jalapeños   \n",
       "274    Salt and freshly ground black pepper to taste  Salt and black pepper   \n",
       "332             Salt and freshly ground black pepper  Salt and black pepper   \n",
       "347             Salt and freshly ground black pepper  Salt and black pepper   \n",
       "362                   Salt and freshly ground pepper        Salt and pepper   \n",
       "\n",
       "       qty  range_end      unit name_no_punct  \\\n",
       "index                                           \n",
       "253    2.0        3.0  teaspoon          True   \n",
       "274    0.0        0.0       nan          True   \n",
       "332    0.0        0.0       nan          True   \n",
       "347    0.0        0.0       nan          True   \n",
       "362    0.0        0.0       nan          True   \n",
       "\n",
       "                                        input_parsed            name_parsed  \\\n",
       "index                                                                         \n",
       "253                 2 to 3 teaspoons minced jalapeño              jalapeños   \n",
       "274    salt and freshly ground black pepper to taste  salt and black pepper   \n",
       "332             salt and freshly ground black pepper  salt and black pepper   \n",
       "347             salt and freshly ground black pepper  salt and black pepper   \n",
       "362                   salt and freshly ground pepper        salt and pepper   \n",
       "\n",
       "      unit_parsed  check_name  \n",
       "index                          \n",
       "253      teaspoon       False  \n",
       "274           nan       False  \n",
       "332           nan       False  \n",
       "347           nan       False  \n",
       "362           nan       False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Check that the input column contains the ingredient name and also the unit if it has one\n",
    "def check_if_name_and_unit(input, name, unit):\n",
    "    name_regexp = f\"{name}\\w*\"\n",
    "    unit_regexp = f\"{unit}\\w*\"\n",
    "\n",
    "    match = re.search(name_regexp, input)\n",
    "\n",
    "    if not match:\n",
    "        return False\n",
    "    # Check that the unit isnt nan\n",
    "    if not unit==unit:\n",
    "        print(unit)\n",
    "        match = re.search(unit_regexp, input)\n",
    "        if not match:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "ing_df[\"check_name\"] = ing_df.apply(lambda x: check_if_name_and_unit(x['input_parsed'], x['name_parsed'], x['unit_parsed']), axis=1)\n",
    "ing_df.head()\n",
    "\n",
    "# Check a few of the entries where it is false\n",
    "ing_df[~ing_df[\"check_name\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above I confirm that the input column has the name and unit in it. This helps to sift out data containing errors.\n",
    "It also means sifting out data such as input = \"2 teaspoons of jalapeños\" where the name has been given as name = \"jalapeño\".\n",
    "It will remove this data, but I do not believe it will have a significant impact as whether the ingredient is plural or\n",
    "not it will still be in the same position in the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing all the rows where the name and unit are not included in the input, it leaves 170302 rows\n"
     ]
    }
   ],
   "source": [
    "ing_df = ing_df[ing_df[\"check_name\"]]\n",
    "print(f\"After removing all the rows where the name and unit are not included in the input, it leaves {len(ing_df)} rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating labels\n",
    "\n",
    "For the unit and ingredient labels, I wil need to convert \"2 tablespoons and 1 teaspoon of white sugar\" into [0 0 0 0 0 0 1 1] for instance.\n",
    "\n",
    "Now that the data is processed, this should not be too difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert a string of multiple words into a binary array with 1 for each word in \n",
    "# Match is and 0 where any other words are.\n",
    "def create_label(inp, match):\n",
    "    match_regexp = f\"{match}\\w*\"\n",
    "    match_num_words = len(match.split(\" \"))                  \n",
    "    match_replaced = re.sub(match_regexp, \" MATCH \", inp)\n",
    "    match_replaced = re.sub(\"\\s+\", \" \" ,match_replaced)\n",
    "    match_replaced = re.sub(\"(\\s$)|(^\\s)\", \"\", match_replaced)\n",
    "    word_array = match_replaced.split(\" \")\n",
    "    label = []\n",
    "    for word in word_array:\n",
    "        if word == \"MATCH\":\n",
    "            label.extend([1]*match_num_words)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return label\n",
    "        \n",
    "ing_df[\"name_label\"] = ing_df.apply(lambda x: create_label(x['input_parsed'], x['name_parsed']), axis=1)\n",
    "ing_df[\"unit_label\"] = ing_df.apply(lambda x: create_label(x['input_parsed'], x['unit_parsed']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_parsed</th>\n",
       "      <th>name_label</th>\n",
       "      <th>unit_label</th>\n",
       "      <th>qty</th>\n",
       "      <th>range_end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>1 tablespoon fresh lemon juice</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81373</th>\n",
       "      <td>pinch salt</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125617</th>\n",
       "      <td>2 garlic cloves , minced</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78967</th>\n",
       "      <td>4 fillets of black sea bass , skins on about 1...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174044</th>\n",
       "      <td>2 medium garlic cloves , thinly sliced</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63679</th>\n",
       "      <td>1 / 4 teaspoon grated lemon rind</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99317</th>\n",
       "      <td>8 peppercorns</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16585</th>\n",
       "      <td>1 teaspoon sugar</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129986</th>\n",
       "      <td>6 boneless , skinless chicken breasts</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133513</th>\n",
       "      <td>2 tablespoons extra virgin olive oil</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_parsed  \\\n",
       "index                                                       \n",
       "14592                      1 tablespoon fresh lemon juice   \n",
       "81373                                          pinch salt   \n",
       "125617                           2 garlic cloves , minced   \n",
       "78967   4 fillets of black sea bass , skins on about 1...   \n",
       "174044             2 medium garlic cloves , thinly sliced   \n",
       "...                                                   ...   \n",
       "63679                    1 / 4 teaspoon grated lemon rind   \n",
       "99317                                       8 peppercorns   \n",
       "16585                                    1 teaspoon sugar   \n",
       "129986              6 boneless , skinless chicken breasts   \n",
       "133513               2 tablespoons extra virgin olive oil   \n",
       "\n",
       "                                           name_label  \\\n",
       "index                                                   \n",
       "14592                                 [0, 0, 1, 1, 1]   \n",
       "81373                                          [0, 1]   \n",
       "125617                                [0, 1, 0, 0, 0]   \n",
       "78967   [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "174044                          [0, 0, 1, 0, 0, 0, 0]   \n",
       "...                                               ...   \n",
       "63679                           [0, 0, 0, 0, 0, 1, 1]   \n",
       "99317                                          [0, 1]   \n",
       "16585                                       [0, 0, 1]   \n",
       "129986                             [0, 0, 0, 0, 1, 1]   \n",
       "133513                             [0, 0, 1, 1, 1, 1]   \n",
       "\n",
       "                                           unit_label   qty  range_end  \n",
       "index                                                                   \n",
       "14592                                 [0, 1, 0, 0, 0]  1.00        0.0  \n",
       "81373                                          [1, 0]  1.00        0.0  \n",
       "125617                                [0, 0, 0, 0, 0]  2.00        0.0  \n",
       "78967   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  4.00        0.0  \n",
       "174044                          [0, 0, 0, 1, 0, 0, 0]  2.00        0.0  \n",
       "...                                               ...   ...        ...  \n",
       "63679                           [0, 0, 0, 1, 0, 0, 0]  0.25        0.0  \n",
       "99317                                          [0, 0]  8.00        0.0  \n",
       "16585                                       [0, 1, 0]  1.00        0.0  \n",
       "129986                             [0, 0, 0, 0, 0, 0]  6.00        0.0  \n",
       "133513                             [0, 0, 0, 0, 0, 0]  2.00        0.0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ing_df_labelled = ing_df[[\"input_parsed\", 'name_label', \"unit_label\", \"qty\", \"range_end\"]]\n",
    "ing_df_labelled.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look pretty good. The next stage would be to encode the text so that it is also numeric.\n",
    "\n",
    "To do this I need to:\n",
    "- Identify the vocabulary from the corpus\n",
    "- Convert the words into their numeric representation\n",
    "- Pad the sequences so that all the inputs are the same length\n",
    "- Download the pretrained word embeddings (using Glove 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in word_index: 7501\n",
      "word_index: [('<OOV>', 1), ('1', 2), ('2', 3), (',', 4), ('/', 5), ('cup', 6), ('4', 7), ('tablespoons', 8), ('3', 9), ('teaspoon', 10), ('or', 11), ('chopped', 12), ('and', 13), ('to', 14), ('salt', 15), ('cups', 16), ('oil', 17), ('tablespoon', 18), ('fresh', 19), ('pepper', 20)]\n",
      "\n",
      "(170302, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from more_itertools import take\n",
    "\n",
    "# Initialize the Tokenizer class, making sure to keep , . and /\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='!\"#$%&()*+-:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(ing_df_labelled[\"input_parsed\"])\n",
    "\n",
    "# Print the length of the word index\n",
    "word_index_example = take(20, tokenizer.word_index.items())\n",
    "word_index = tokenizer.word_index\n",
    "print(f'number of words in word_index: {len(word_index)}')\n",
    "\n",
    "# Print the word index\n",
    "print(f'word_index: {word_index_example}')\n",
    "print()\n",
    "\n",
    "# Generate and pad the sequences\n",
    "sequences = tokenizer.texts_to_sequences(ing_df_labelled[\"input_parsed\"])\n",
    "\n",
    "X = pad_sequences(sequences, padding='post')\n",
    "y_name = pad_sequences(ing_df_labelled[\"name_label\"], padding=\"post\")\n",
    "y_unit = pad_sequences(ing_df_labelled[\"unit_label\"], padding=\"post\")\n",
    "y_qty = np.expand_dims(np.asarray(ing_df_labelled[\"qty\"].values), axis=-1)\n",
    "y_range = np.expand_dims(np.asarray(ing_df_labelled[\"range_end\"].values), axis=-1)\n",
    "\n",
    "y_qtys = np.concatenate((y_qty, y_range), axis=-1)\n",
    "\n",
    "# Combine these for jointly training the model\n",
    "y_name_ex = np.expand_dims(y_name, axis=1)\n",
    "y_unit_ex = np.expand_dims(y_unit, axis=1)\n",
    "print(y_qtys.shape)\n",
    "\n",
    "y_combined = np.concatenate((y_name_ex, y_unit_ex), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredient sentence is 2 tablespoons rum\n",
      "X is [  3   8 342   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "y_name is [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_unit is [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "# Check a random index to make sure everything has worked\n",
    "idx = random.randint(0,20000)\n",
    "rand_ing = ing_df.loc[ing_df.index.values[idx], \"input_parsed\"]\n",
    "print(f\"Ingredient sentence is {rand_ing}\")\n",
    "print(f\"X is {X[idx]}\")\n",
    "print(f\"y_name is {y_name[idx]}\")\n",
    "print(f\"y_unit is {y_unit[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great so we have a training database! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "To save time training my own embeddings, I will use the ones already trained in the Glove version 6 embeddings. These were created by stanford on a much larger corpus. Naturally some words might not exsist in this corpus, so these will be replaced with the OOV token.\n",
    "\n",
    "Below I download and save the dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download and extract the word embeddings\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Open the word embeddings \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5915 words (1586 misses)\n",
      "Some examples of the words not included are: ['<OOV>', 'href', 'fraîche', 'puréed', 'deribbed', 'calamata', '1½', 'pimentón', 'cherrystone', 'zested', 'dukkah', 'debearded', 'frisée', 'niçoise', 'medjool', 'comice', 'florettes', 'unblanched', 'borlotti', 'porcinis']\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(word_index) + 1\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "oov_words = []\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        oov_words.append(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "print(f\"Some examples of the words not included are: {oov_words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these words from the word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I looked at a longer list I would see a lot of large numeric values. It appears that larger numbers do not have embeddings. To improve the model in the future, all numbers could be given a numeric token and all words containing non-english characters could be replaced for their english equivilant. THen any remaining words would be replaced with the OOV token. Then the embedding layer could be partially fixed and train embeddings for the new tokens.\n",
    "\n",
    "Because of the current approach, OOV words are replaced with a 0 so they are treated the same as the padding. This is unlikely to have a significant effect, except for when the ingredient name is at the end of the phrase. Lets see how it does anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import Constant\n",
    "# Create the embedding layer, make it not trainable and fix the embedding values\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    input_length=X.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Calculation\n",
    "\n",
    "If my model was to output all 1's or all 0's for the output, what would the success rate be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline f1 scores for y_unit are 0.02337087910892176\n",
      "The baseline f1 scores for y_name are 0.055756400440218165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "baseline_y_unit = f1_score(y_unit, np.ones(y_unit.shape), average=\"micro\")\n",
    "baseline_y_name = f1_score(y_name, np.ones(y_name.shape), average=\"micro\")\n",
    "\n",
    "print(f\"The baseline f1 scores for y_unit are {baseline_y_unit}\")\n",
    "print(f\"The baseline f1 scores for y_name are {baseline_y_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 07:35:56.335429: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "\n",
    "f1_score_name = tfa.metrics.F1Score(\n",
    "    num_classes = X.shape[1],\n",
    "    average = \"micro\",\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "f1_score_unit = tfa.metrics.F1Score(\n",
    "    num_classes = X.shape[1],\n",
    "    average = \"micro\",\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "\n",
    "def name_f1_score(y_true_comb, y_pred_comb):\n",
    "    y_true_name, y_true_unit = tf.split(y_true_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    y_pred_name, y_pred_unit = tf.split(y_pred_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    return f1_score_name(tf.squeeze(y_true_name), tf.squeeze(y_pred_name))\n",
    "\n",
    "def unit_f1_score(y_true_comb, y_pred_comb):\n",
    "    y_true_name, y_true_unit = tf.split(y_true_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    y_pred_name, y_pred_unit = tf.split(y_pred_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    return f1_score_unit(tf.squeeze(y_true_unit), tf.squeeze(y_pred_unit))\n",
    "\n",
    "\n",
    "def name_percent_part_correct(y_true_comb, y_pred_comb):\n",
    "    y_true_name, y_true_unit = tf.split(y_true_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    y_pred_name, y_pred_unit = tf.split(y_pred_comb, num_or_size_splits=[1, 1], axis=1)\n",
    "    \n",
    "    metric = percent_part_correct(tf.squeeze(y_true_name), tf.squeeze(y_pred_name))\n",
    "    return metric\n",
    "\n",
    "def percent_part_correct(y_true, y_pred):\n",
    "    y_round = tf.cast(tf.math.round(y_pred), tf.bool)\n",
    "    y_true_bool = tf.cast(y_true, tf.bool)\n",
    "    total_values = tf.gather(tf.shape(y_true), 0)\n",
    "    \n",
    "    # Returns a boolean tensor where the elements match\n",
    "    equal = tf.math.logical_and(y_true_bool, y_round)\n",
    "    only_ones = tf.logical_and(equal, y_true_bool)\n",
    "    reduced_eq = tf.reduce_any(only_ones, 1)\n",
    "    total_part_correct = tf.reduce_sum(tf.cast(reduced_eq, tf.int64))\n",
    "    percent_part_correct = tf.math.divide_no_nan(tf.cast(total_part_correct, tf.float32), tf.cast(total_values, tf.float32))\n",
    "    return percent_part_correct\n",
    "\n",
    "def qty_mse(y_true, y_pred):\n",
    "    y_true_qty, y_true_range = tf.split(y_true, num_or_size_splits=[1, 1], axis=-1)\n",
    "    y_pred_qty, y_pred_range = tf.split(y_pred, num_or_size_splits=[1, 1], axis=-1)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    mse_error = mse(y_true_qty, y_pred_qty)\n",
    "    return mse_error\n",
    "\n",
    "def range_mse(y_true, y_pred):\n",
    "    y_true_qty, y_true_range = tf.split(y_true, num_or_size_splits=[1, 1], axis=-1)\n",
    "    y_pred_qty, y_pred_range = tf.split(y_pred, num_or_size_splits=[1, 1], axis=-1)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    mse_error = mse(y_true_range, y_pred_range)\n",
    "    return mse_error\n",
    "\n",
    "\n",
    "# Test execution of custom metric\n",
    "y_p = tf.constant([[[0, 0.51, 0], [0.51, 0, 0]], [[1, 0, 0], [0, 0, 0]]], dtype=tf.float32)\n",
    "y_t = tf.constant([[[0, 1, 0], [1, 0, 0]], [[0, 0, 0], [0, 0, 0]]], dtype=tf.float32)\n",
    "\n",
    "print(name_percent_part_correct(y_combined, y_combined).numpy())\n",
    "\n",
    "# Test the custom f1_score\n",
    "print(unit_f1_score(y_t, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_combined, test_size=0.2, random_state=42)\n",
    "_, _, y_qty_train, y_qty_test = train_test_split(X, y_qty, test_size=0.2, random_state=42)\n",
    "_, _, y_range_train, y_range_test = train_test_split(X, y_range, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 60, 100)           750200    \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 60, 128)          84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirecti  (None, 60, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               983168    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,916,664\n",
      "Trainable params: 1,166,464\n",
      "Non-trainable params: 750,200\n",
      "_________________________________________________________________\n",
      "10643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lstm_dim = 64\n",
    "dense_dim = 128\n",
    "output_dim = X.shape[1]*2\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Model Definition with LSTM\n",
    "model_head = tf.keras.Sequential([\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim, return_sequences=True)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(dense_dim, activation='relu')\n",
    "], name=\"model_head\")\n",
    "# Print the model summary\n",
    "model_head.summary()\n",
    "\n",
    "save_freq = int(5.0*(len(X_train)/BATCH_SIZE))\n",
    "print(save_freq)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('models/model{epoch:08d}.ckpt', save_freq=save_freq, save_weights_only=\"true\")\n",
    "                                                                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 60)]         0           []                               \n",
      "                                                                                                  \n",
      " model_head (Sequential)        (None, 128)          1916664     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " model_tail_1 (Sequential)      (None, 2, 60)        15480       ['model_head[0][0]']             \n",
      "                                                                                                  \n",
      " model_tail_2 (Sequential)      (None, 1)            129         ['model_head[0][0]']             \n",
      "                                                                                                  \n",
      " model_tail_3 (Sequential)      (None, 1)            129         ['model_head[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,932,402\n",
      "Trainable params: 1,182,202\n",
      "Non-trainable params: 750,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tail_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(output_dim, activation='sigmoid'), \n",
    "    tf.keras.layers.Reshape((2, X.shape[1]))\n",
    "], name=\"model_tail_1\")\n",
    "\n",
    "model_tail_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='relu')\n",
    "], name=\"model_tail_2\")\n",
    "\n",
    "model_tail_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='relu')\n",
    "], name=\"model_tail_3\")\n",
    "\n",
    "model_input = tf.keras.layers.Input(shape=[X.shape[1]])\n",
    "features = model_head(model_input)\n",
    "model_output1 = model_tail_1(features)\n",
    "model_output2 = model_tail_2(features)\n",
    "model_output3 = model_tail_3(features)\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=[model_output1, model_output2, model_output3])\n",
    "\n",
    "TAIL1_WEIGHT = 4\n",
    "TAIL2_WEIGHT = 1\n",
    "# Weight is the ratio of positive samples so that the loss is of the same magnitude as the QTY loss\n",
    "TAIL3_WEIGHT = float(len(y_range_train)) / sum([1.0 for i in y_range_train if i > 0.01]) \n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss={\"model_tail_1\": \"binary_crossentropy\", \"model_tail_2\": \"huber\", \"model_tail_3\": \"mse\"},\n",
    "    loss_weights={\"model_tail_1\": TAIL1_WEIGHT, \"model_tail_2\": TAIL2_WEIGHT, \"model_tail_3\": TAIL3_WEIGHT},\n",
    "    metrics={\"model_tail_1\": [name_f1_score, unit_f1_score, name_percent_part_correct], \"model_tail_2\": \"mse\", \"model_tail_3\": \"mse\"}\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.58343558282209\n"
     ]
    }
   ],
   "source": [
    "print(TAIL3_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2129/2129 [==============================] - 223s 102ms/step - loss: 80.3023 - model_tail_1_loss: 0.0315 - model_tail_2_loss: 0.6573 - model_tail_3_loss: 0.9514 - model_tail_1_name_f1_score: 0.8692 - model_tail_1_unit_f1_score: 0.9067 - model_tail_1_name_percent_part_correct: 0.7349 - model_tail_2_mse: 176.3011 - model_tail_3_mse: 0.9514 - val_loss: 54.9397 - val_model_tail_1_loss: 0.0157 - val_model_tail_2_loss: 0.5300 - val_model_tail_3_loss: 0.6502 - val_model_tail_1_name_f1_score: 0.8678 - val_model_tail_1_unit_f1_score: 0.9060 - val_model_tail_1_name_percent_part_correct: 0.9461 - val_model_tail_2_mse: 226.6915 - val_model_tail_3_mse: 0.6502\n",
      "Epoch 2/50\n",
      "2129/2129 [==============================] - 213s 100ms/step - loss: 80.0294 - model_tail_1_loss: 0.0134 - model_tail_2_loss: 0.4562 - model_tail_3_loss: 0.9514 - model_tail_1_name_f1_score: 0.8679 - model_tail_1_unit_f1_score: 0.9061 - model_tail_1_name_percent_part_correct: 0.9713 - model_tail_2_mse: 125.9799 - model_tail_3_mse: 0.9514 - val_loss: 54.7901 - val_model_tail_1_loss: 0.0114 - val_model_tail_2_loss: 0.3979 - val_model_tail_3_loss: 0.6502 - val_model_tail_1_name_f1_score: 0.8683 - val_model_tail_1_unit_f1_score: 0.9062 - val_model_tail_1_name_percent_part_correct: 0.9831 - val_model_tail_2_mse: 195.3327 - val_model_tail_3_mse: 0.6502\n",
      "Epoch 3/50\n",
      "2129/2129 [==============================] - 218s 103ms/step - loss: 81.0982 - model_tail_1_loss: 0.0113 - model_tail_2_loss: 0.4480 - model_tail_3_loss: 0.9644 - model_tail_1_name_f1_score: 0.8687 - model_tail_1_unit_f1_score: 0.9065 - model_tail_1_name_percent_part_correct: 0.9839 - model_tail_2_mse: 125.0164 - model_tail_3_mse: 0.9644 - val_loss: 54.8304 - val_model_tail_1_loss: 0.0108 - val_model_tail_2_loss: 0.4405 - val_model_tail_3_loss: 0.6502 - val_model_tail_1_name_f1_score: 0.8691 - val_model_tail_1_unit_f1_score: 0.9068 - val_model_tail_1_name_percent_part_correct: 0.9874 - val_model_tail_2_mse: 200.5602 - val_model_tail_3_mse: 0.6502\n",
      "Epoch 4/50\n",
      "2129/2129 [==============================] - 216s 101ms/step - loss: 79.9361 - model_tail_1_loss: 0.0103 - model_tail_2_loss: 0.3755 - model_tail_3_loss: 0.9514 - model_tail_1_name_f1_score: 0.8696 - model_tail_1_unit_f1_score: 0.9071 - model_tail_1_name_percent_part_correct: 0.9884 - model_tail_2_mse: 106.7990 - model_tail_3_mse: 0.9514 - val_loss: 54.7740 - val_model_tail_1_loss: 0.0101 - val_model_tail_2_loss: 0.3870 - val_model_tail_3_loss: 0.6502 - val_model_tail_1_name_f1_score: 0.8701 - val_model_tail_1_unit_f1_score: 0.9075 - val_model_tail_1_name_percent_part_correct: 0.9891 - val_model_tail_2_mse: 191.5484 - val_model_tail_3_mse: 0.6502\n",
      "Epoch 5/50\n",
      "2126/2129 [============================>.] - ETA: 0s - loss: 79.8590 - model_tail_1_loss: 0.0099 - model_tail_2_loss: 0.3538 - model_tail_3_loss: 0.9507 - model_tail_1_name_f1_score: 0.8706 - model_tail_1_unit_f1_score: 0.9079 - model_tail_1_name_percent_part_correct: 0.9901 - model_tail_2_mse: 100.4645 - model_tail_3_mse: 0.9507"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "models; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8v/d50_nc2n7kj916nj0mct09f40000gn/T/ipykernel_4665/3651003838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, [y_train, y_qty_train, y_range_train], batch_size=BATCH_SIZE, \n\u001b[0;32m----> 2\u001b[0;31m                               epochs=NUM_EPOCHS, validation_data=(X_test, [y_test, y_qty_test, y_range_test]), callbacks= [checkpoint_callback])\n\u001b[0m",
      "\u001b[0;32m~/Documents/ingredients_2_vec/venv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ingredients_2_vec/venv/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m   \"\"\"\n\u001b[0;32m--> 511\u001b[0;31m   \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: models; No such file or directory"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, [y_train, y_qty_train, y_range_train], batch_size=BATCH_SIZE, \n",
    "                              epochs=NUM_EPOCHS, validation_data=(X_test, [y_test, y_qty_test, y_range_test]), callbacks= [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('./final_model.pb', custom_objects= {\"embedding\": embedding_layer, \"name_f1_score\": name_f1_score, \"unit_f1_score\": unit_f1_score, \"name_percent_part_correct\": name_percent_part_correct, \"qty_mse\": qty_mse, \"range_mse\": range_mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('output_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_graphs(history, strings):\n",
    "    fig = go.Figure()\n",
    "    x = list(range(NUM_EPOCHS))\n",
    "    for string in strings:\n",
    "        fig.add_scatter(x=x, y=history.history[string],\n",
    "                        name=string)\n",
    "        fig.add_scatter(x=x, y=history.history['val_'+string],\n",
    "                        name='val_'+string)\n",
    "    fig.update_traces(mode='lines+markers')# hoverinfo='text+name+y', \n",
    "    fig.update_layout(legend=dict(y=0.5, traceorder='reversed', font_size=16), \n",
    "                    title=\"Training Results\")\n",
    "    fig.show()\n",
    "\n",
    "plot_graphs(model.history, [\"model_tail_1_name_f1_score\", \"model_tail_1_unit_f1_score\"])\n",
    "plot_graphs(model.history, [\"loss\", \"model_tail_1_loss\", \"model_tail_2_loss\", \"model_tail_3_loss\"])\n",
    "plot_graphs(model.history, ['model_tail_1_name_percent_part_correct'])\n",
    "plot_graphs(model.history, ['model_tail_2_qty_mse', 'model_tail_2_range_mse'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loss graph can be used to adjust the weights on each of the contributing losses based on when each tail starts to overfit, ideally they will all start to overfit at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Success and Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_combined_model(n_samples, qty_failure=False, has_range=False):\n",
    "    start_index = 100000\n",
    "    subset_size = 10000\n",
    "    X_sentences = ing_df_labelled.loc[ing_df_labelled.index[start_index:start_index+subset_size], \"input_parsed\"].values\n",
    "    X_input = X[start_index:start_index+subset_size]\n",
    "\n",
    "    y_name_subset = y_name[start_index:start_index+subset_size]\n",
    "    y_unit_subset = y_unit[start_index:start_index+subset_size]\n",
    "    y_qty_subset  = y_qty[start_index:start_index+subset_size]\n",
    "    y_range_subset = y_range[start_index:start_index+subset_size]\n",
    "    pred_y_subset = model.predict(X[start_index:start_index+subset_size])\n",
    "    \n",
    "\n",
    "    pred_y_name = pred_y_subset[0][:, 0, :]\n",
    "    pred_y_unit = pred_y_subset[0][:, 1, :]\n",
    "    pred_y_qty  = pred_y_subset[1]\n",
    "    pred_y_range = pred_y_subset[2]\n",
    "        \n",
    "    rounded_yp_name = np.rint(np.asarray(pred_y_name))\n",
    "    rounded_yp_unit = np.rint(np.asarray(pred_y_unit))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        if qty_failure:\n",
    "            if np.abs(pred_y_qty[i] - y_qty_subset[i]) > 3:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        if has_range:\n",
    "            if y_range_subset[i] > 0.01:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        print(f\"Sentence: {X_sentences[i]}\")\n",
    "        print()\n",
    "        print(f\"y_name: {binary_mask_to_words(y_name_subset[i], X_sentences[i])}\")\n",
    "        print(f\"pred_name: {binary_mask_to_words(rounded_yp_name[i], X_sentences[i])}\")\n",
    "        print()\n",
    "        print(f\"y_unit: {binary_mask_to_words(y_unit_subset[i], X_sentences[i])}\")\n",
    "        print(f\"pred_unit: {binary_mask_to_words(rounded_yp_unit[i], X_sentences[i])}\")\n",
    "        print()\n",
    "        print(f\"y_qty: {y_qty_subset[i]}\")\n",
    "        print(f\"pred_qty: {pred_y_qty[i]}\")\n",
    "        print()\n",
    "        print(f\"y_range: {y_range_subset[i]}\")\n",
    "        print(f\"pred_range: {pred_y_range[i]}\")\n",
    "        print()\n",
    "\n",
    "# Applies a binary mask to a sentence and returns the strings. Used for calculating the \n",
    "def binary_mask_to_words(binary_mask, sentence):\n",
    "    split_sentence = sentence.split(\" \")\n",
    "    idxs = np.where(binary_mask > 0.5)\n",
    "    y_words = \" \".join([split_sentence[int(idx)] for idx in idxs[0]])\n",
    "    return y_words\n",
    "    \n",
    "test_combined_model(10000, has_range=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting failure (or maybe a success?):\n",
    "\n",
    "3 / 4 cup plus 2 tablespoons sugar -> Model: 9.98 Tablespoons sugar Label: 3/4 cup sugar\n",
    "\n",
    "3/4 cups is actually 10 tablespoons, so the model answer is just as good as the true answer. If only it were able to add the additional 2 teaspoons as well!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify fail cases\n",
    "\n",
    "Because the data I have used has had the labels converted and because it is text data (I find it complicated and havent used it before), I want to see where the model has failed. This will helo to refine the preprocesing and dta filtering. It might be errors from the differences between what different labelers consider to be the ingredient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(fail_pass: str, part_correct: bool, name_unit: str, limit: int):\n",
    "    start_index = 120000\n",
    "    subset_size = 1000\n",
    "    X_sentences = ing_df_labelled.loc[ing_df_labelled.index[start_index:start_index+subset_size], \"input_parsed\"].values\n",
    "    X_input = X[start_index:start_index+subset_size]\n",
    "    if name_unit == \"name\":\n",
    "        y_subset = y_name[start_index:start_index+subset_size]\n",
    "    else:\n",
    "        y_subset = y_unit[start_index:start_index+subset_size]\n",
    "        \n",
    "    pred_y_subset = model_lstm.predict(X[start_index:start_index+subset_size])\n",
    "    \n",
    "    if name_unit == \"name\":\n",
    "        pred_y_subset = pred_y_subset[:, 0, :]\n",
    "    else:\n",
    "        pred_y_subset = pred_y_subset[:, 1, :]\n",
    "        \n",
    "    rounded_y_pred = np.rint(np.asarray(pred_y_subset))\n",
    "    \n",
    "    printed = 0\n",
    "    # Print any errors\n",
    "    for i in range(subset_size):\n",
    "        if printed > limit - 1:\n",
    "            return\n",
    "        if fail_pass == \"fail\":\n",
    "            if part_correct:\n",
    "                # Checks that there is at least one matching positive and then checks that there is not a perfect match.\n",
    "                if (np.asarray(y_subset[i]) & rounded_y_pred[i].astype(int)).any() and not (np.asarray(y_subset[i]) == rounded_y_pred[i]).all():\n",
    "                    print(f\"Sentence: {X_sentences[i]}\")\n",
    "                    print(f\"y: {binary_mask_to_words(y_subset[i], X_sentences[i])}\")\n",
    "                    print(f\"pred_y: {binary_mask_to_words(rounded_y_pred[i], X_sentences[i])}\")\n",
    "                    print()\n",
    "                    printed += 1\n",
    "            else:\n",
    "                # Checks that there is at least one word that is a name or unit\n",
    "                # Then finds out where the overlap is between the y_true and the y_pred, bthen combines it with the y_pred array using and \n",
    "                # to remove the matches where both are 0\n",
    "                # Then checks that there are no 1's in this array, meaning a compelte error\n",
    "                if np.any(np.asarray(y_subset[i])):\n",
    "                    mask = (np.asarray(y_subset[i]) == rounded_y_pred[i]) & np.asarray(y_subset[i])\n",
    "                    if not mask.any():\n",
    "                        print(f\"Sentence: {X_sentences[i]}\")\n",
    "                        print(f\"y: {binary_mask_to_words(y_subset[i], X_sentences[i])}\")\n",
    "                        print(f\"pred_y: {binary_mask_to_words(rounded_y_pred[i], X_sentences[i])}\")\n",
    "                        print()\n",
    "                        printed += 1\n",
    "\n",
    "        if fail_pass == \"pass\":\n",
    "            if (np.asarray(y_subset[i]) == rounded_y_pred[i]).all():\n",
    "                print(f\"Sentence: {X_sentences[i]}\")\n",
    "                print()\n",
    "                printed += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Name cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_examples(\"fail\", True, \"name\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are two kind of errors. Errors where the model has clearly gotten it wrong:\n",
    "\n",
    "1 pinch sea salt or fleur de sel -> Model Output: \"Sea Salt Or\"\n",
    "\n",
    "And others which are open to interpretation or where the labeller has gotten it wrong:\n",
    "\n",
    "1 red grapefruit , peeled and segmented , optional -> Model Output: \"red grapefruit\" Label: \"grapefruit\"\n",
    "\n",
    "1 / 4 cup vadouvan exotique spice mix -> Model Output: \"vadouvan exotique spice mix\" Label: \"spice mix\"\n",
    "\n",
    "2 teaspoons aged balsamic vinegar -> Model Output: \"balsamic vinegar\" Label: \"vinegar\"\n",
    "\n",
    "I cannot think of any straightforward ways to seperate these mistakes, however, I can create a metric that includes the above examples as successful predictions and then have a look at the ones that are not these type of errors. This would be useful as a better predictor of model performance I believe.\n",
    "\n",
    "This metric would include any prediction with an overlap with the Label a successful prediction and then give a percentage of the data set that fall into this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_examples(\"fail\", False, \"name\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aboe example of errors, it looks like the algorithm is actually better than the human labeller in this instance.\n",
    "\n",
    "crackers or sliced cucumber , for serving -> Model: crackers, Label: cucumber\n",
    "1 / 2 cup soaking water from the apricots , as needed - > Model: water Label: apricots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Unit Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Successful Predictions\")\n",
    "print_examples(\"pass\", False, \"unit\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unsuccessful Predictions\")\n",
    "print_examples(\"fail\", False, \"unit\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unsuccessful predictions where part of the prediction is correct\")\n",
    "print_examples(\"fail\", True, \"unit\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
